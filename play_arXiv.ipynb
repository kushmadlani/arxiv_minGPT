{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a word-level GPT on ML arXiv abstracts\n",
    "\n",
    "The arXiv dataset on [Kaggle](https://www.kaggle.com/Cornell-University/arxiv) provides meta-data on thousands of papers published over the past decades. In this post, we take all the abstracts from papers in the field of Machine Learning (or related fields) then train GPT on it. We use Andrej Karpathy's [minGPT](https://github.com/karpathy/minGPT) - a PyTorch re-implementation of OpenAI's [GPT](https://github.com/openai/gpt-3) that \"tries to be small, clean, interpretable and educational\" (it is.)\n",
    "\n",
    "We train our model a single GPU available on Google Colab and feed it some prompts, which we then get it to predict an entire Machine Learning abstract! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ArDMxQOlEmXF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive # import drive from google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "wkWBF-4PEr-R",
    "outputId": "61da01dc-2752-4007-a95a-0ff797154f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"/content/drive\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    "\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary to ensure that paths are correct for importing data from the google drive folder\n",
    "# insert correct root for minGPT code\n",
    "minGPT_DIR = '/minGPT/'\n",
    "%cd $minGPT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fjm_-ItXEkmg"
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dw8-SHM0Ekmj"
   },
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8sQ6n49UEkml"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "pd.set_option('float_format', '{:f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data, using `yield` below to avoid memory problems with the huge json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r40gHkWHEkmo"
   },
   "outputs": [],
   "source": [
    "file_path = 'arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "def get_metadata():\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just look at papers from the past 10 years and select those part of the three categories arXiv tags AI papers in:\n",
    "- 'cs.AI': 'Artificial Intelligence'\n",
    "- 'cs.LG': 'Machine Learning'\n",
    "- 'stat.ML': 'Machine Learning'\n",
    "\n",
    "That gets us 4673 abstracts to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AuQ0tfJ7Ekmq"
   },
   "outputs": [],
   "source": [
    "ai_list = ['cs.AI','cs.LG','stat.ML']\n",
    "abstracts = []\n",
    "\n",
    "metadata = get_metadata()\n",
    "# loop over all papers\n",
    "for paper in metadata:\n",
    "    # extract single paper\n",
    "    paper_dict = json.loads(paper)\n",
    "    version = paper_dict.get('versions')\n",
    "    category = paper_dict.get('categories')\n",
    "    try:\n",
    "        try:\n",
    "            year = int(paper_dict.get('journal-ref')[-4:])    ### Example Format: \"Phys.Rev.D76:013009,2007\"\n",
    "        except:\n",
    "            year = int(paper_dict.get('journal-ref')[-5:-1])    ### Example Format: \"Phys.Rev.D76:013009,(2007)\"\n",
    "\n",
    "        if any(ele in category for ele in ai_list) and 2010<year<2021:\n",
    "            abstracts.append(paper_dict.get('abstract'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Xk8O3lLbEkms",
    "outputId": "8bf07b64-dd59-4496-9266-3c99b434e30b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4673"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to preprocess the abstracts as follows, after which we get a corpus of 857,479 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l85N2rzqEkmv"
   },
   "outputs": [],
   "source": [
    "# string whitespace at end of words, replace new lines by space and add 'end of sentence' token\n",
    "f = lambda x: x.strip().replace(\"\\n\",\" \") + \" #EOS\"\n",
    "abstracts = [f(x) for x in abstracts]\n",
    "# seperate all words and punctuation\n",
    "abstracts = [re.findall(r\"[\\w']+|[.,!?;]\", x) for x in abstracts]\n",
    "# turn list of lists in to single list\n",
    "abstracts = [j for i in abstracts for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5g16yynPFfCo",
    "outputId": "86bb2f3e-9216-4ce3-901d-b8dc30a1aea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857479"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wE3xuxF7Ekmy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        words = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(words)\n",
    "        print('data has %d words, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(words) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(words) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every word to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        # See https://github.com/karpathy/minGPT/blob/master/play_char.ipynb for\n",
    "        # explainer of Dataset construction\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Dataset object defined we can load our dataset with a block size of 128 appropriate since the average abstract in arXiv 122 words long (see prev [post](https://kushmadlani.github.io/arxiv-eda/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AkAtjXLCEkm2",
    "outputId": "a7392043-7359-4582-f6cf-0a2bf23439f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 857479 words, 25921 unique.\n"
     ]
    }
   ],
   "source": [
    "block_size = 128 # sets spatial extent of the model for its context\n",
    "train_dataset = WordDataset(abstracts, block_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a GPT! In the Character level transformer example Karpathy wrote up he built a 'GPT-1' with 8 layers and 8 heads - here we halve that to 4 layers and 4 attention heads so to able to train it on a Colab GPU (I guess we call this 'GPT-0.5')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qM-PXfm0Ekm5",
    "outputId": "daca1c51-757e-4d70-a693-3a2f9d7f4a20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/27/2020 19:49:54 - INFO - mingpt.model -   number of parameters: 1.646387e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=4, n_head=4, n_embd=256)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "n_fZvsOMEkm7",
    "outputId": "d556de44-3d8b-44ea-9f12-b5065d4930a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 6698: train loss 1.35257. lr 3.000110e-04: 100%|██████████| 6699/6699 [24:41<00:00,  4.52it/s]\n",
      "epoch 2 iter 6698: train loss 0.94379. lr 6.000000e-05: 100%|██████████| 6699/6699 [24:45<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=128, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=256*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained! Let's generate some Machine Learning abstracts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "CnhBhCsEEkm9",
    "outputId": "6288be78-2c80-4052-9221-1cf3e2eb074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper discusses the effect of the design and implementation of a case study . EOS Graph Neural Networks GNNs achieve remarkable performance in graph data classification tasks . In graph classification , each node of node information from labeled nodes measured nodes in a graph are connected by many , each graph represents the goal of node embedding space . Multiple graph embedding aims to create a similarity graph by representing the different graph each path graph in each graph . This information represents the embedding by learning a knowledge graph by node as the network . The goal is to design a similarity graph embedding that represents a set of entities and the entities in the graph . The nodes are generated using graph embedding techniques , which represent graph embedding methods with embedding methods , on nodes using graphs . graphs are based on the space of nodes in a\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some word-level abstracts\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = ['This', 'paper', 'discusses']\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 150, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ' '.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "J4aso47UJUvJ",
    "outputId": "a8ded4c9-62a8-4440-e4d1-2293fa97c069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce the first time algorithm to compute activity of a given graph in a graph and then using graph clustering . We give examples that in our algorithm enjoys a faster convergence rate than previous methods . We furthermore show that it is a way to address problems with different edges that are important to be used for graph clustering . We define new rank six block models , low rank , low rank , and we are able to provide an improved numerical implementation of our model . These findings are extremely sensible and useful matrix factorization NMF . EOS This paper describes a recent , much discussion about the concepts of the SP theory of intelligence , with its realisation in the SP machine both outlined in the article may help to simplify and work in the design of autonomous robots that may assist in the design of autonomous robots\n"
     ]
    }
   ],
   "source": [
    "context = ['We', 'introduce', 'the']\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 150, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ' '.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Zh7eHTACUvEa",
    "outputId": "6bcb03c4-fb1e-468c-f281-854ab250ed7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our work has focused on the first large class of relevant problems , where the problem is to identify , and evaluate the single , rank , and all algorithms simultaneously satisfy the two properties and time constraints , respectively . The experimental result shows the interest of the proposed solving high dimensional G odel's original paper illustrates the global analysis of analysed , in the context of a link with the whole paper . EOS Many studies claim that an object can be an object , e . g . , geographic query answer set in order to , when instances in a database of interest . Many classes of problems are given . More recently , some attack scenarios rely on node graph based keyword , which aims to select a set of contextual features that are most similar for each context e . g . , for that for every node ,\n"
     ]
    }
   ],
   "source": [
    "context = ['Our', 'work', 'has', 'focused', 'on']\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 200, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ' '.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "ICPF8vooU4dS",
    "outputId": "89d7059d-0c53-4cd2-93a0-2a809fa63b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our work has focused on the use of multi modal social networks and web recommender systems , in which contain heterogeneous information and items . In this paper , we propose a multi modal data embedding framework to detect matches semantically similar contexts in order to their opinions . We show that both methods can be successfully applied to Web and document clustering tasks . EOS In this paper we study the problem of finding the rating of two and , the rating score for a given time . In particular , we use the following the following questions 1 The given answer is a certain item such that the set of at any a certain item we choose one , and use the rating , combined with the answer to answer based . We review the characteristics and compare the baselines in detail to these questions . To this end , we built a deep ranking approach for general and general and statistical analysis of some recent QA methods . EOS We consider the problem of learning a probabilistic domain , agent using data . Given a collection of Chinese e commerce , we allow a posterior over a subset of interest , and\n"
     ]
    }
   ],
   "source": [
    "context = ['Our', 'work', 'has', 'focused', 'on']\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 200, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ' '.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "5rRLxPPEVJlp",
    "outputId": "39a4a7cc-1ba8-4306-b095-ea646f8b700b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper considers the problem of finding a single optimal clustering that minimizes a specific number of disagreements i . e . , the sum of the number of observed missing edges within clusters . The objective of most promising intelligent algorithms appear to be evaluated on the basis of similarity matrix . However , most of the problems have with high probability , that they are designed for the pair of clusters are distinct from observational data . The optimal clustering must pass through a grid like time varying quality . We develop a new algorithm to learn K coordinate dictionaries , with dimensions m_k times p_k up to estimation error varepsilon_k is shown to be max_ k in K mathcal O m_kp_k 3 varepsilon_k 2 . EOS Understanding the causes of crime is a longstanding issue in researcher's agenda . While it is a hard task to extract causality from data\n"
     ]
    }
   ],
   "source": [
    "context = ['This', 'paper', 'considers']\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 150, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ' '.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "arxiv_abstract.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
